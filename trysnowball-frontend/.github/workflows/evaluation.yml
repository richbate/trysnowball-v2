name: ðŸ§ª System Evaluation

on:
  pull_request:
    branches: [ main, develop ]
    types: [opened, synchronize, reopened]
  push:
    branches: [ main ]

concurrency:
  group: evaluation-${{ github.ref }}
  cancel-in-progress: true

jobs:
  evaluate:
    name: Evaluate System
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: ðŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ðŸ“¦ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: 'npm'

      - name: ðŸ“¦ Install dependencies
        run: npm ci --prefer-offline --no-audit

      - name: ðŸ§ª Run fast evaluation (PR)
        if: github.event_name == 'pull_request'
        run: npm run evaluate:ci
        continue-on-error: false

      - name: ðŸ“Š Generate evaluation summary
        if: always()
        run: |
          if [ -f "reports/junit.xml" ]; then
            npm run eval:summary || echo "Summary generation failed"
          else
            echo "No test results to summarize"
          fi

      - name: ðŸ“„ Upload evaluation artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-results-${{ github.run_id }}
          path: |
            reports/
            coverage/
          retention-days: 7

      - name: ðŸ’¬ Comment PR with results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            
            // Try to read the evaluation summary
            let summaryContent = '## ðŸ§ª Evaluation Results\n\n';
            
            try {
              const summaryPath = path.join('reports', 'eval-summary.md');
              if (fs.existsSync(summaryPath)) {
                summaryContent = fs.readFileSync(summaryPath, 'utf8');
              } else {
                summaryContent += 'âš ï¸ Evaluation summary not found. Check the logs for details.';
              }
            } catch (error) {
              summaryContent += `âŒ Failed to read evaluation results: ${error.message}`;
            }
            
            // Add workflow link
            summaryContent += `\n\n---\nðŸ” [View detailed logs](${context.payload.pull_request.html_url}/checks)`;
            
            // Find existing evaluation comment
            const comments = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const botComment = comments.data.find(comment => 
              comment.user.type === 'Bot' && 
              comment.body.includes('ðŸ§ª Evaluation Results')
            );
            
            if (botComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: summaryContent
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: summaryContent
              });
            }

      - name: âŒ Fail if evaluation failed
        if: github.event_name == 'pull_request'
        run: |
          if [ -f "reports/eval-summary.json" ]; then
            # Check if evaluation passed
            if ! jq -e '.deployReady == true' reports/eval-summary.json > /dev/null 2>&1; then
              echo "::error::Evaluation failed - not ready for deployment"
              exit 1
            fi
          else
            echo "::error::No evaluation summary found"
            exit 1
          fi